{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c416485-c7b7-438a-940c-b400104fa6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6999586731336330>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCREATE OR REFRESH MATERIALIZED VIEW gold_customer_order_summary\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAS\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT c.customer_id,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.customer_name,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.date_of_birth,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.telephone,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.email,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.address_line1,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.city,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.state,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.postcode,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       COUNT(DISTINCT o.order_id) as total_orders,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       SUM(o.item_quantity) as total_items_ordered,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       SUM(o.item_quantity * o.price) as total_order_amount\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       FROM LIVE.silver_customers c\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       JOIN LIVE.silver_addresses a ON c.customer_id = a.customer_id\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       JOIN LIVE.silver_orders o ON c.customer_id = o.customer_id\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       WHERE a.__END_AT IS NULL\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       GROUP BY ALL\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2543\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2541\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2542\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2543\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2547\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2548\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:130\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n",
       "\u001B[1;32m    126\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    127\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n",
       "\u001B[1;32m    128\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    129\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    131\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:123\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    121\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n",
       "\u001B[1;32m    122\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 123\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n",
       "\u001B[1;32m    126\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    127\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n",
       "\u001B[1;32m    128\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    129\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:185\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_analysis(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point):\n",
       "\u001B[1;32m    184\u001B[0m     repl_context \u001B[38;5;241m=\u001B[39m get_context()\n",
       "\u001B[0;32m--> 185\u001B[0m     html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39manalyzeAndFormatResult(\n",
       "\u001B[1;32m    186\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(repl_context, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotebookId\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    187\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(repl_context, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotebookPath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    188\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(repl_context, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworkspaceId\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    189\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetDLTAnalysisId()\u001B[38;5;241m.\u001B[39mget(), sql_directive\u001B[38;5;241m.\u001B[39msql())\n",
       "\u001B[1;32m    190\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_last_query:\n",
       "\u001B[1;32m    191\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayHTML(html)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling t.analyzeAndFormatResult.\n",
       ": com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'silver_customers'. This dataset is not defined in the pipeline.\n",
       "If this table is managed by another pipeline, then do not use `dlt.read` / `dlt.readStream` to read the table or prepend the name with the LIVE keyword.\n",
       "\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:13)\n",
       "\tat com.databricks.pipelines.Pipeline.$anonfun$readDltInput$1(Pipeline.scala:635)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n",
       "\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:626)\n",
       "\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:570)\n",
       "\tat com.databricks.pipelines.Pipeline$.readBatchInput(Pipeline.scala:2418)\n",
       "\tat com.databricks.pipelines.SQLPipeline$$anonfun$2.applyOrElse(SQLPipeline.scala:641)\n",
       "\tat com.databricks.pipelines.SQLPipeline$$anonfun$2.applyOrElse(SQLPipeline.scala:615)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.apply(QueryPlan.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.apply(QueryPlan.scala:653)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.applyOrElse(QueryPlan.scala:653)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2342)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1357)\n",
       "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:940)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1357)\n",
       "\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:940)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:379)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1682)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformDownWithSubqueriesAndPruning(QueryPlan.scala:667)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformDownWithSubqueries(QueryPlan.scala:616)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformWithSubqueries(QueryPlan.scala:583)\n",
       "\tat com.databricks.pipelines.SQLPipeline$.analyze(SQLPipeline.scala:615)\n",
       "\tat com.databricks.pipelines.SQLPipeline.$anonfun$analyze$1(SQLPipeline.scala:419)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat com.databricks.pipelines.common.errors.ScopedThreadLocal.runWith(ScopedThreadLocal_Spark.scala:18)\n",
       "\tat com.databricks.pipelines.common.errors.DLTQueryOrigin$.withOrigin(DLTQueryOrigin.scala:153)\n",
       "\tat com.databricks.pipelines.SQLPipeline.analyze(SQLPipeline.scala:419)\n",
       "\tat com.databricks.pipelines.util.CommandsUtils$$anonfun$versionSpecificPlanHandler$1.$anonfun$applyOrElse$9(CommandsUtils_DBR_13_2_Plus.scala:180)\n",
       "\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:1108)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat com.databricks.pipelines.common.errors.ScopedThreadLocal.runWith(ScopedThreadLocal_Spark.scala:18)\n",
       "\tat com.databricks.pipelines.common.errors.DLTQueryOrigin$.withOrigin(DLTQueryOrigin.scala:153)\n",
       "\tat com.databricks.pipelines.DataFrameFunction.invoke(Pipeline.scala:141)\n",
       "\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$5(Pipeline.scala:2495)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n",
       "\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:2329)\n",
       "\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:2493)\n",
       "\tat com.databricks.pipelines.ThreadLocalPipelineContextStack.withContext(Pipeline.scala:244)\n",
       "\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:382)\n",
       "\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:2493)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$1(Pipeline.scala:2491)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n",
       "\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:2480)\n",
       "\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:591)\n",
       "\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:504)\n",
       "\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n",
       "\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:502)\n",
       "\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:486)\n",
       "\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:584)\n",
       "\tat com.databricks.pipelines.graph.Flow.$anonfun$flowFuncResult$1(Flow.scala:259)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:211)\n",
       "\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:210)\n",
       "\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:663)\n",
       "\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:410)\n",
       "\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:401)\n",
       "\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:663)\n",
       "\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:437)\n",
       "\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:437)\n",
       "\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:663)\n",
       "\tat com.databricks.pipelines.graph.FlowResolver.$anonfun$attemptResolveFlow$1(FlowResolver.scala:234)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n",
       "\tat com.databricks.pipelines.graph.FlowResolver.attemptResolveFlow(FlowResolver.scala:229)\n",
       "\tat com.databricks.pipelines.graph.ParallelFlowResolver.$anonfun$resolveInternal$5(FlowResolver.scala:359)\n",
       "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
       "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
       "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.sql.transaction.tahoe.mst.MultiStatementTransactionThreadHelper$.runWithMstTxnId(MultiStatementTransactionThreadHelper.scala:55)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\tSuppressed: com.databricks.pipelines.common.errors.DLTQueryOrigin$DLTQueryOriginWrapper\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling t.analyzeAndFormatResult.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'silver_customers'. This dataset is not defined in the pipeline.\nIf this table is managed by another pipeline, then do not use `dlt.read` / `dlt.readStream` to read the table or prepend the name with the LIVE keyword.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:13)\n\tat com.databricks.pipelines.Pipeline.$anonfun$readDltInput$1(Pipeline.scala:635)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:626)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:570)\n\tat com.databricks.pipelines.Pipeline$.readBatchInput(Pipeline.scala:2418)\n\tat com.databricks.pipelines.SQLPipeline$$anonfun$2.applyOrElse(SQLPipeline.scala:641)\n\tat com.databricks.pipelines.SQLPipeline$$anonfun$2.applyOrElse(SQLPipeline.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.apply(QueryPlan.scala:657)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.apply(QueryPlan.scala:653)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.applyOrElse(QueryPlan.scala:653)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2342)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1357)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:940)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1357)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:940)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1682)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformDownWithSubqueriesAndPruning(QueryPlan.scala:667)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformDownWithSubqueries(QueryPlan.scala:616)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformWithSubqueries(QueryPlan.scala:583)\n\tat com.databricks.pipelines.SQLPipeline$.analyze(SQLPipeline.scala:615)\n\tat com.databricks.pipelines.SQLPipeline.$anonfun$analyze$1(SQLPipeline.scala:419)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.pipelines.common.errors.ScopedThreadLocal.runWith(ScopedThreadLocal_Spark.scala:18)\n\tat com.databricks.pipelines.common.errors.DLTQueryOrigin$.withOrigin(DLTQueryOrigin.scala:153)\n\tat com.databricks.pipelines.SQLPipeline.analyze(SQLPipeline.scala:419)\n\tat com.databricks.pipelines.util.CommandsUtils$$anonfun$versionSpecificPlanHandler$1.$anonfun$applyOrElse$9(CommandsUtils_DBR_13_2_Plus.scala:180)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:1108)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.pipelines.common.errors.ScopedThreadLocal.runWith(ScopedThreadLocal_Spark.scala:18)\n\tat com.databricks.pipelines.common.errors.DLTQueryOrigin$.withOrigin(DLTQueryOrigin.scala:153)\n\tat com.databricks.pipelines.DataFrameFunction.invoke(Pipeline.scala:141)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$5(Pipeline.scala:2495)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:2329)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:2493)\n\tat com.databricks.pipelines.ThreadLocalPipelineContextStack.withContext(Pipeline.scala:244)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:382)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:2493)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$1(Pipeline.scala:2491)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:2480)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:591)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:504)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:502)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:486)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:584)\n\tat com.databricks.pipelines.graph.Flow.$anonfun$flowFuncResult$1(Flow.scala:259)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:211)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:210)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:663)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:410)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:401)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:663)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:437)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:437)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:663)\n\tat com.databricks.pipelines.graph.FlowResolver.$anonfun$attemptResolveFlow$1(FlowResolver.scala:234)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.FlowResolver.attemptResolveFlow(FlowResolver.scala:229)\n\tat com.databricks.pipelines.graph.ParallelFlowResolver.$anonfun$resolveInternal$5(FlowResolver.scala:359)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MultiStatementTransactionThreadHelper$.runWithMstTxnId(MultiStatementTransactionThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: com.databricks.pipelines.common.errors.DLTQueryOrigin$DLTQueryOriginWrapper\n"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-6999586731336330>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCREATE OR REFRESH MATERIALIZED VIEW gold_customer_order_summary\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAS\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT c.customer_id,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.customer_name,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.date_of_birth,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.telephone,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       c.email,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.address_line1,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.city,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.state,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       a.postcode,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       COUNT(DISTINCT o.order_id) as total_orders,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       SUM(o.item_quantity) as total_items_ordered,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       SUM(o.item_quantity * o.price) as total_order_amount\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       FROM LIVE.silver_customers c\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       JOIN LIVE.silver_addresses a ON c.customer_id = a.customer_id\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       JOIN LIVE.silver_orders o ON c.customer_id = o.customer_id\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       WHERE a.__END_AT IS NULL\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m       GROUP BY ALL\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2543\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2541\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2542\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2543\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2547\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2548\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:130\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    127\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    128\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    129\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:123\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    121\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n\u001B[1;32m    122\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 123\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    127\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    128\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    129\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:185\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_analysis(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point):\n\u001B[1;32m    184\u001B[0m     repl_context \u001B[38;5;241m=\u001B[39m get_context()\n\u001B[0;32m--> 185\u001B[0m     html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39manalyzeAndFormatResult(\n\u001B[1;32m    186\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(repl_context, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotebookId\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    187\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(repl_context, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotebookPath\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    188\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(repl_context, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworkspaceId\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    189\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetDLTAnalysisId()\u001B[38;5;241m.\u001B[39mget(), sql_directive\u001B[38;5;241m.\u001B[39msql())\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_last_query:\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayHTML(html)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py:327\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    325\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    329\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    333\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling t.analyzeAndFormatResult.\n: com.databricks.pipelines.common.errors.DLTSparkException: [DATASET_NOT_DEFINED] Failed to read dataset 'silver_customers'. This dataset is not defined in the pipeline.\nIf this table is managed by another pipeline, then do not use `dlt.read` / `dlt.readStream` to read the table or prepend the name with the LIVE keyword.\n\tat com.databricks.pipelines.api.GraphErrors$.datasetNotDefinedError(GraphErrors.scala:13)\n\tat com.databricks.pipelines.Pipeline.$anonfun$readDltInput$1(Pipeline.scala:635)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.Pipeline.readDltInput(Pipeline.scala:626)\n\tat com.databricks.pipelines.Pipeline.com$databricks$pipelines$Pipeline$$readBatchInput(Pipeline.scala:570)\n\tat com.databricks.pipelines.Pipeline$.readBatchInput(Pipeline.scala:2418)\n\tat com.databricks.pipelines.SQLPipeline$$anonfun$2.applyOrElse(SQLPipeline.scala:641)\n\tat com.databricks.pipelines.SQLPipeline$$anonfun$2.applyOrElse(SQLPipeline.scala:615)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.apply(QueryPlan.scala:657)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.apply(QueryPlan.scala:653)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anon$3.applyOrElse(QueryPlan.scala:653)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2342)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1357)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:940)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1357)\n\tat org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.mapChildren(basicLogicalOperators.scala:940)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.Filter.mapChildren(basicLogicalOperators.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)\n\tat org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1682)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformDownWithSubqueriesAndPruning(QueryPlan.scala:667)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformDownWithSubqueries(QueryPlan.scala:616)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformWithSubqueries(QueryPlan.scala:583)\n\tat com.databricks.pipelines.SQLPipeline$.analyze(SQLPipeline.scala:615)\n\tat com.databricks.pipelines.SQLPipeline.$anonfun$analyze$1(SQLPipeline.scala:419)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.pipelines.common.errors.ScopedThreadLocal.runWith(ScopedThreadLocal_Spark.scala:18)\n\tat com.databricks.pipelines.common.errors.DLTQueryOrigin$.withOrigin(DLTQueryOrigin.scala:153)\n\tat com.databricks.pipelines.SQLPipeline.analyze(SQLPipeline.scala:419)\n\tat com.databricks.pipelines.util.CommandsUtils$$anonfun$versionSpecificPlanHandler$1.$anonfun$applyOrElse$9(CommandsUtils_DBR_13_2_Plus.scala:180)\n\tat com.databricks.pipelines.Pipeline$DatasetBuilderImpl.$anonfun$query$2(Pipeline.scala:1108)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.pipelines.common.errors.ScopedThreadLocal.runWith(ScopedThreadLocal_Spark.scala:18)\n\tat com.databricks.pipelines.common.errors.DLTQueryOrigin$.withOrigin(DLTQueryOrigin.scala:153)\n\tat com.databricks.pipelines.DataFrameFunction.invoke(Pipeline.scala:141)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$5(Pipeline.scala:2495)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:317)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:315)\n\tat com.databricks.pipelines.Pipeline$.recordFrameProfile(Pipeline.scala:2329)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$4(Pipeline.scala:2493)\n\tat com.databricks.pipelines.ThreadLocalPipelineContextStack.withContext(Pipeline.scala:244)\n\tat com.databricks.pipelines.Pipeline.withContext(Pipeline.scala:382)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$3(Pipeline.scala:2493)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.pipelines.Pipeline$$anon$2.$anonfun$call$1(Pipeline.scala:2491)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.Pipeline$$anon$2.call(Pipeline.scala:2480)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.call(Flow.scala:591)\n\tat com.databricks.pipelines.graph.FlowFunction.$anonfun$callWithCache$1(Flow.scala:504)\n\tat scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:110)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache(Flow.scala:502)\n\tat com.databricks.pipelines.graph.FlowFunction.callWithCache$(Flow.scala:486)\n\tat com.databricks.pipelines.graph.FlowFunction$$anon$3.callWithCache(Flow.scala:584)\n\tat com.databricks.pipelines.graph.Flow.$anonfun$flowFuncResult$1(Flow.scala:259)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult(Flow.scala:211)\n\tat com.databricks.pipelines.graph.Flow.flowFuncResult$(Flow.scala:210)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.flowFuncResult(Flow.scala:663)\n\tat com.databricks.pipelines.graph.Flow.failure(Flow.scala:410)\n\tat com.databricks.pipelines.graph.Flow.failure$(Flow.scala:401)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.failure(Flow.scala:663)\n\tat com.databricks.pipelines.graph.Flow.resolved(Flow.scala:437)\n\tat com.databricks.pipelines.graph.Flow.resolved$(Flow.scala:437)\n\tat com.databricks.pipelines.graph.UnresolvedFlow.resolved(Flow.scala:663)\n\tat com.databricks.pipelines.graph.FlowResolver.$anonfun$attemptResolveFlow$1(FlowResolver.scala:234)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$3(DltApiUsageLogging.scala:71)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$4(DltApiUsageLogging.scala:83)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:299)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:295)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionContext(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.withAttributionTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperationWithResultTags(DltApiUsageLogging.scala:31)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordOperation(DltApiUsageLogging.scala:31)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.$anonfun$recordPipelinesOperation$1(DltApiUsageLogging.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.pipelines.graph.DltApiUsageLogging$.recordPipelinesOperation(DltApiUsageLogging.scala:59)\n\tat com.databricks.pipelines.graph.FlowResolver.attemptResolveFlow(FlowResolver.scala:229)\n\tat com.databricks.pipelines.graph.ParallelFlowResolver.$anonfun$resolveInternal$5(FlowResolver.scala:359)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.sql.transaction.tahoe.mst.MultiStatementTransactionThreadHelper$.runWithMstTxnId(MultiStatementTransactionThreadHelper.scala:55)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:114)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:91)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:158)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:161)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: com.databricks.pipelines.common.errors.DLTQueryOrigin$DLTQueryOriginWrapper\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "CREATE OR REFRESH MATERIALIZED VIEW gold_customer_order_summary\n",
    "AS\n",
    "SELECT c.customer_id,\n",
    "       c.customer_name,\n",
    "       c.date_of_birth,\n",
    "       c.telephone,\n",
    "       c.email,\n",
    "       a.address_line_1,\n",
    "       a.city,\n",
    "       a.state,\n",
    "       a.postcode,\n",
    "       COUNT(DISTINCT o.order_id) as total_orders,\n",
    "       SUM(o.quantity) as total_items_ordered,\n",
    "       SUM(o.quantity * o.price) as total_order_amount\n",
    "       FROM LIVE.silver_customers c\n",
    "       JOIN LIVE.silver_addresses a ON c.customer_id = a.customer_id\n",
    "       JOIN LIVE.silver_orders o ON c.customer_id = o.customer_id\n",
    "       WHERE a.__END_AT IS NULL\n",
    "       GROUP BY ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "415de1fc-6f11-4a5a-9f49-b697627748bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6999586731336330,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CreateCustomerOrderSummary",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}